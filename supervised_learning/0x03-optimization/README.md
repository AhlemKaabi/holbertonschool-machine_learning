# Optimization

In machine learning, Optimizing algorithms is essential to find the best "way" to get efficient results.

**Holberton School project - Machine learning Specialization**

In this project we apply diffrent optimization algorithms used in machine learning.

### **Learning Objectives**


* What is a hyperparameter?
* How and why do you normalize your input data?
* What is a saddle point?
* What is stochastic gradient descent?
* What is mini-batch gradient descent?
* What is a moving average? How do you implement it?
* What is gradient descent with momentum? How do you implement it?
* What is RMSProp? How do you implement it?
* What is Adam optimization? How do you implement it?
* What is learning rate decay? How do you implement it?
* What is batch normalization? How do you implement it?
