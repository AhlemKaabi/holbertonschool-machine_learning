# Optimization

In machine learning, Optimizing algorithms is essential to find the best "way" to get efficient results.

In this project we apply diffrent optimization algorithms used in machine learning.

### **Learning Objectives**


* What is a hyperparameter?
* How and why do you normalize your input data?
* What is a saddle point?
* What is stochastic gradient descent?
* What is mini-batch gradient descent?
* What is a moving average? How do you implement it?
* What is gradient descent with momentum? How do you implement it?
* What is RMSProp? How do you implement it?
* What is Adam optimization? How do you implement it?
* What is learning rate decay? How do you implement it?
* What is batch normalization? How do you implement it?

## **Blog - Gradient Descent Optimization Techniques for Machine Learning Algorithms**

* check my blog on [Medium](https://ahlemkaabi1412.medium.com/gradient-descent-optimization-techniques-for-machine-learning-algorithms-f0c19de7023a ) or check it as post in my [LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:6901531689718222848/ )

***Holberton School project - Machine learning Specialization***